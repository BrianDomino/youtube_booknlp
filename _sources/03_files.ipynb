{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "changed-kennedy",
   "metadata": {},
   "source": [
    "# <center>The Output Files</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-netherlands",
   "metadata": {},
   "source": [
    "<center>Dr. W.J.B. Mattingly</center>\n",
    "\n",
    "<center>Smithsonian Data Science Lab and United States Holocaust Memorial Museum</center>\n",
    "\n",
    "<center>August 2021</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-harvard",
   "metadata": {},
   "source": [
    "## Covered in this Chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-sunglasses",
   "metadata": {},
   "source": [
    "1) The .tokens file<br>\n",
    "2) The .entities file<br>\n",
    "3) The .quote file<br>\n",
    "4) The .supersense file<br>\n",
    "5) The .book file<br>\n",
    "6) The .book.html file<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-juvenile",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-martin",
   "metadata": {},
   "source": [
    "In the last chapter, we looked at how to create a BookNLP pipeline and process a book or longer document. The goal of that process was to generate a collection of files within an output directory. In our case, we stored our files in \"data/harry_potter\". Within this repo, you will be able to examine the output files, but rather than making you switch between this textbook and the repo, I thought I would present the files in this chapter with screenshots. With that said, let's begin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-coffee",
   "metadata": {},
   "source": [
    "## The .tokens file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-perception",
   "metadata": {},
   "source": [
    "The very first file that we should analyze is the .tokens file. Essentially, this is a tab separated value file (TSV). A good way to think about a TSV is as a CSV where tabs are used to separate tabular data, rather than commas. Essentially, this is a dataset that can be viewed and analyzed in Excel. The very first line of the file will look something like this:\n",
    "\n",
    "paragraph_ID\tsentence_ID\ttoken_ID_within_sentence\ttoken_ID_within_document\tword\tlemma\tbyte_onset\tbyte_offset\tPOS_tag\tfine_POS_tag\tdependency_relation\tsyntactic_head_ID\tevent\n",
    "\n",
    "As this can be a bit difficult to parse, I am going to load it up as a TSV file through Pandas so we can analyze it a bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fallen-tracker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_ID</th>\n",
       "      <th>sentence_ID</th>\n",
       "      <th>token_ID_within_sentence</th>\n",
       "      <th>token_ID_within_document</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>byte_onset</th>\n",
       "      <th>byte_offset</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>fine_POS_tag</th>\n",
       "      <th>dependency_relation</th>\n",
       "      <th>syntactic_head_ID</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Harry</td>\n",
       "      <td>Harry</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Potter</td>\n",
       "      <td>Potter</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>12</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CC</td>\n",
       "      <td>cc</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>4</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Sorcerer</td>\n",
       "      <td>Sorcerer</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>poss</td>\n",
       "      <td>8</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99396</th>\n",
       "      <td>3030</td>\n",
       "      <td>6972</td>\n",
       "      <td>12</td>\n",
       "      <td>99396</td>\n",
       "      <td>summer</td>\n",
       "      <td>summer</td>\n",
       "      <td>439721</td>\n",
       "      <td>439727</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>npadvmod</td>\n",
       "      <td>99388</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99397</th>\n",
       "      <td>3030</td>\n",
       "      <td>6972</td>\n",
       "      <td>13</td>\n",
       "      <td>99397</td>\n",
       "      <td>....</td>\n",
       "      <td>....</td>\n",
       "      <td>439727</td>\n",
       "      <td>439731</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>punct</td>\n",
       "      <td>99386</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99398</th>\n",
       "      <td>3030</td>\n",
       "      <td>6972</td>\n",
       "      <td>14</td>\n",
       "      <td>99398</td>\n",
       "      <td>\\t</td>\n",
       "      <td>439731</td>\n",
       "      <td>439732</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>''</td>\n",
       "      <td>punct</td>\n",
       "      <td>99386</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99399</th>\n",
       "      <td>3031</td>\n",
       "      <td>6973</td>\n",
       "      <td>0</td>\n",
       "      <td>99399</td>\n",
       "      <td>THE</td>\n",
       "      <td>the</td>\n",
       "      <td>439734</td>\n",
       "      <td>439737</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>99400</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99400</th>\n",
       "      <td>3031</td>\n",
       "      <td>6973</td>\n",
       "      <td>1</td>\n",
       "      <td>99400</td>\n",
       "      <td>END</td>\n",
       "      <td>END</td>\n",
       "      <td>439738</td>\n",
       "      <td>439741</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>99398</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99401 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       paragraph_ID  sentence_ID  token_ID_within_sentence  \\\n",
       "0                 0            0                         0   \n",
       "1                 0            0                         1   \n",
       "2                 0            0                         2   \n",
       "3                 0            0                         3   \n",
       "4                 0            0                         4   \n",
       "...             ...          ...                       ...   \n",
       "99396          3030         6972                        12   \n",
       "99397          3030         6972                        13   \n",
       "99398          3030         6972                        14   \n",
       "99399          3031         6973                         0   \n",
       "99400          3031         6973                         1   \n",
       "\n",
       "       token_ID_within_document      word     lemma  byte_onset byte_offset  \\\n",
       "0                             0     Harry     Harry           0           5   \n",
       "1                             1    Potter    Potter           6          12   \n",
       "2                             2       and       and          13          16   \n",
       "3                             3       the       the          17          20   \n",
       "4                             4  Sorcerer  Sorcerer          21          29   \n",
       "...                         ...       ...       ...         ...         ...   \n",
       "99396                     99396    summer    summer      439721      439727   \n",
       "99397                     99397      ....      ....      439727      439731   \n",
       "99398                     99398        \\t    439731      439732       PUNCT   \n",
       "99399                     99399       THE       the      439734      439737   \n",
       "99400                     99400       END       END      439738      439741   \n",
       "\n",
       "      POS_tag fine_POS_tag dependency_relation syntactic_head_ID event  \n",
       "0       PROPN          NNP            compound                 1     O  \n",
       "1       PROPN          NNP               nsubj                12     O  \n",
       "2       CCONJ           CC                  cc                 1     O  \n",
       "3         DET           DT                 det                 4     O  \n",
       "4       PROPN          NNP                poss                 8     O  \n",
       "...       ...          ...                 ...               ...   ...  \n",
       "99396    NOUN           NN            npadvmod             99388     O  \n",
       "99397   PUNCT            .               punct             99386     O  \n",
       "99398      ''        punct               99386                 O   NaN  \n",
       "99399     DET           DT                 det             99400     O  \n",
       "99400   PROPN          NNP               nsubj             99398     O  \n",
       "\n",
       "[99401 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/harry_potter/harry_potter.tokens\", delimiter=\"\\t\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-ethernet",
   "metadata": {},
   "source": [
    "If you don't know what the block of code above does, please do not be concerned. We will not be dealing with Pandas in this textbook. If you are interested in Pandas, though, I have a free textbook on it entitled <a href=\"pandas.pythonhumanities.com\">Introduction to Pandas</a>.\n",
    "\n",
    "As you can see from the output above, we have something that looks like Excel, or tabular data. Let's break this down a bit and explain what each column represents:\n",
    "\n",
    "- paragraph_ID = the index of the paragraph, starting at paragraph 1 being 0 and moving up to 3031 in our case.\n",
    "- sentence_ID = same as the paragraph_ID, but with sentences\n",
    "- token_ID_within_sentence = same as a the two above, but with a token count by sentence, resetting with each sentence.\n",
    "- token_ID_within_document = same as above, but where tokens keep going up in value throughout the whole document, starting at 0 and ending, in our case, at 99400.\n",
    "- word = this is the raw text of the word\n",
    "- lemma = this is the root of the word\n",
    "- byte_onset = think of this as the start character index\n",
    "- byte_offset - think of this as the concluding character index\n",
    "- POS_tag = the Part of Speech (based on spaCy)\n",
    "- fine_POS_tag = a more granular understanding of the Part of Speech\n",
    "- dependency_relation = this is equivalent to spaCy's dep tag.\n",
    "- syntactic_head_ID = This points to the head of the current token so that you can understand how a token relates to other words in the sentence\n",
    "- event = this tells you if the toke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-width",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "environmental-manner",
   "metadata": {},
   "source": [
    "## The .entities File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-custody",
   "metadata": {},
   "source": [
    "## The .quote File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-grant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ambient-layout",
   "metadata": {},
   "source": [
    "## The .supersense file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-purpose",
   "metadata": {},
   "source": [
    "## The .book File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-footage",
   "metadata": {},
   "source": [
    "## The .book.html File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-short",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "collected-hostel",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-oliver",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "interstate-nightmare",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-poker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-zealand",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-shaft",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
