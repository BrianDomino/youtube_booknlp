{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dominant-celebration",
   "metadata": {},
   "source": [
    "# <center>Character Analysis</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-citation",
   "metadata": {},
   "source": [
    "<center>Dr. W.J.B. Mattingly</center>\n",
    "\n",
    "<center>Smithsonian Data Science Lab and United States Holocaust Memorial Museum</center>\n",
    "\n",
    "<center>August 2021</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-france",
   "metadata": {},
   "source": [
    "## Covered in this Chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-bones",
   "metadata": {},
   "source": [
    "1) <br>\n",
    "2) <br>\n",
    "3) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-giant",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-greeting",
   "metadata": {},
   "source": [
    "This chapter is dedicated to analyzing the characters contained within the .book file. As you may recall from the last chapter, this is a JSON file. A lot of what I will cover here, can be found in the BookNLP repository, specifically in the Google Colab Jupyter Notebook. I am, however, making some modifications to the code there to make it a bit more useful for varying circumstances. I will specifically show you how to use this restructured data to pose narrow questions about characters in a text.\n",
    "\n",
    "The following functions and imports will be necessary for this chapter. They allow us to load up the JSON data from the .book file and count the occurrences of certain things found within the .book file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regional-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cultural-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc(filename):\n",
    "    with open(filename) as file:\n",
    "        data=json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3a5be76-9d26-4815-871d-d268d5a462cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counter_from_dependency_list(dep_list):\n",
    "    counter=Counter()\n",
    "    for token in dep_list:\n",
    "        term=token[\"w\"]\n",
    "        tokenGlobalIndex=token[\"i\"]\n",
    "        counter[term]+=1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-tourist",
   "metadata": {},
   "source": [
    "Now that we have successfully created these functions, let's go ahead and load up our JSON data from the .book file. We can do this with the function above that we created called \"proc\". Essentially, this loads and parses the JSON file for us using the JSON library that comes standard with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "secure-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=proc(\"data/harry_potter/harry_potter.book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-movement",
   "metadata": {},
   "source": [
    "Now that we have loaded the data, we can start to analyze it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-physiology",
   "metadata": {},
   "source": [
    "## Analyzing the Characters (From BookNLP Repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-frame",
   "metadata": {},
   "source": [
    "If you have had a chance to look at the Google Colab notebook provided by BookNLP, this function will look similar. I have made some modifications to the code presented there so that we can do a bit more with it. In the notebook, the original code printed off char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "animal-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_character_data(data, printTop):\n",
    "    character_data = {}\n",
    "    for character in data[\"characters\"]:\n",
    "\n",
    "        agentList=character[\"agent\"]\n",
    "        patientList=character[\"patient\"]\n",
    "        possList=character[\"poss\"]\n",
    "        modList=character[\"mod\"]\n",
    "\n",
    "        character_id=character[\"id\"]\n",
    "        count=character[\"count\"]\n",
    "\n",
    "        referential_gender_distribution=referential_gender_prediction=\"unknown\"\n",
    "\n",
    "        if character[\"g\"] is not None and character[\"g\"] != \"unknown\":\n",
    "            referential_gender_distribution=character[\"g\"][\"inference\"]\n",
    "            referential_gender=character[\"g\"][\"argmax\"]\n",
    "\n",
    "        mentions=character[\"mentions\"]\n",
    "        proper_mentions=mentions[\"proper\"]\n",
    "        max_proper_mention=\"\"\n",
    "        \n",
    "        poss_items = []\n",
    "        agent_items = []\n",
    "        patient_items = []\n",
    "        mod_items = []\n",
    "    \n",
    "        # just print out information about named characters\n",
    "        if len(mentions[\"proper\"]) > 0:\n",
    "            max_proper_mention=mentions[\"proper\"][0][\"n\"]\n",
    "            for k, v in get_counter_from_dependency_list(possList).most_common(printTop):\n",
    "                poss_items.append((v,k))\n",
    "                \n",
    "            for k, v in get_counter_from_dependency_list(agentList).most_common(printTop):\n",
    "                agent_items.append((v,k))     \n",
    "\n",
    "            for k, v in get_counter_from_dependency_list(patientList).most_common(printTop):\n",
    "                patient_items.append((v,k))     \n",
    "\n",
    "            for k, v in get_counter_from_dependency_list(modList).most_common(printTop):\n",
    "                mod_items.append((v,k))  \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            # print(character_id, count, max_proper_mention, referential_gender)\n",
    "            character_data[character_id] = {\"id\": character_id,\n",
    "                                  \"count\": count,\n",
    "                                  \"max_proper_mention\": max_proper_mention,\n",
    "                                  \"referential_gender\": referential_gender,\n",
    "                                  \"possList\": poss_items,\n",
    "                                  \"agentList\": agent_items,\n",
    "                                  \"patientList\": patient_items,\n",
    "                                  \"modList\": mod_items\n",
    "                                 }\n",
    "                                \n",
    "    return character_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e4126b-e32d-4fa1-b556-8dce43ff6a48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cfff6968-0ede-46eb-ba4a-38b71e07a99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 98, 'count': 2029, 'max_proper_mention': 'Harry', 'referential_gender': 'he/him/his', 'possList': [(19, 'head'), (15, 'eyes'), (12, 'parents'), (12, 'cupboard'), (10, 'life'), (10, 'hand'), (9, 'aunt'), (8, 'mind'), (7, 'heart'), (7, 'uncle')], 'agentList': [(91, 'said'), (46, 'had'), (39, 'know'), (22, 'felt'), (22, 'saw'), (21, 'got'), (21, 'going'), (21, 'thought'), (18, 'heard'), (18, 'looked')], 'patientList': [(10, 'told'), (5, 'take'), (5, 'asked'), (4, 'kill'), (4, 'reminded'), (4, 'stop'), (4, 'got'), (4, 'tell'), (3, 'took'), (3, 'saw')], 'modList': [(8, 'sure'), (5, 'able'), (4, 'famous'), (3, 'glad'), (2, 'name'), (2, 'special'), (2, 'surprised'), (2, 'baby'), (2, 'stupid'), (2, 'afraid')]}\n"
     ]
    }
   ],
   "source": [
    "character_data = create_character_data(data, 10)\n",
    "print (character_data[98])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd96b3-8d94-46b1-bb78-b59b229bd3d2",
   "metadata": {},
   "source": [
    "## Parsing Verb Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f50f8f81-b0fd-454a-a7fd-49864129d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_verb_usage(data, analysis=[\"agent\", \"patient\"]):\n",
    "    new_analysis = []\n",
    "    for item in analysis:\n",
    "        if item == \"agent\":\n",
    "            new_analysis.append(\"agentList\")\n",
    "        elif item == \"patient\":\n",
    "            new_analysis.append(\"patientList\")\n",
    "    main_agents = {}\n",
    "    main_patients = {}\n",
    "    for character in character_data:\n",
    "        temp_data = character_data[character]\n",
    "        for item in new_analysis:\n",
    "            for verb in temp_data[item]:\n",
    "                if item == \"patientList\":\n",
    "                    if verb[1] not in main_agents:\n",
    "                        main_agents[verb[1]] = [(character, temp_data[\"max_proper_mention\"])]\n",
    "                    else:\n",
    "                        main_agents[verb[1]].append((character, temp_data[\"max_proper_mention\"]))\n",
    "                elif item == \"agentList\":\n",
    "                    if verb[1] not in main_patients:\n",
    "                        main_patients[verb[1]] = [(character, temp_data[\"max_proper_mention\"])]\n",
    "                    else:\n",
    "                        main_patients[verb[1]].append((character, temp_data[\"max_proper_mention\"]))\n",
    "    verb_usage = {\"agent\": main_agents,\n",
    "                 \"patient\": main_patients}\n",
    "    return verb_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b4239b12-2ca2-41c9-b998-21d67a190c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_data = find_verb_usage(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091574fa-7645-4fc2-804a-36a300966b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
